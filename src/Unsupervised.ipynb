{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "large-prince",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-52f5c0524753>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneral_helper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_predictions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\cap_3\\health_recommender\\src\\helper\\keras_predictions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_net\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# display options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# custom helper functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper.general_helper import *\n",
    "from helper.keras_predictions import *\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# view plotly in jupyter \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shared-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "played-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = df.head(1).imUrl.values[0]\n",
    "\n",
    "im = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-dealing",
   "metadata": {},
   "source": [
    "#### Reduce Sparcity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hourly-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notna()]\n",
    "\n",
    "# select reviewer and product values\n",
    "customers = df['reviewerID'].value_counts()\n",
    "products = df['asin'].value_counts()\n",
    "\n",
    "# filter by 10 reviews per product per customer, products with 20 or more reviews\n",
    "customers = customers[customers >= 10]\n",
    "products = products[products >= 20]\n",
    "\n",
    "# merge dataframe\n",
    "df = df.merge(pd.DataFrame({'reviewerID': customers.index})).merge(pd.DataFrame({'asin': products.index}))\n",
    "\n",
    "\n",
    "# select test size\n",
    "n = 39601\n",
    "\n",
    "# shuffle dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split into train and test set\n",
    "df_train = df[:-n]\n",
    "df_test = df[-n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-titanium",
   "metadata": {},
   "source": [
    "#### Recommending off KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df.pivot_table(index='reviewerID', columns='asin', values='overall', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, random_state=0).fit(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = kmeans.predict(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pred_dict = defaultdict(int)\n",
    "for user, pred in zip(pivot_df.index, preds):\n",
    "    user_pred_dict[user] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df.reviewerID.apply(lambda x: user_pred_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = dict(df.groupby('class').mean().overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preds'] = df['class'].apply(lambda x: pred_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = df[['overall', 'preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(test_values['overall'], test_values['preds']))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = abs(df['overall'] - df['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = sns.distplot(error, kde=True)\n",
    "plt.axvline(np.median(error),color='r', linestyle='--')\n",
    "plt.title('Distribution of Error: KMeans', fontsize=18)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plt.xlabel('Error', fontsize=16)\n",
    "plt.ylabel('Count', fontsize=16)\n",
    "# plt.savefig(r\"../images/kmeans_error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = AgglomerativeClustering(n_clusters=20)\n",
    "preds = alg.fit_predict(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pred_dict = defaultdict(int)\n",
    "for user, pred in zip(pivot_df.index, preds):\n",
    "    user_pred_dict[user] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df.reviewerID.apply(lambda x: user_pred_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = dict(df.groupby('review').mean().overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preds'] = df['class'].apply(lambda x: pred_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = df[['overall', 'preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(test_values['overall'], test_values['preds']))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = abs(df['overall'] - df['preds'])\n",
    "\n",
    "figure = sns.distplot(error, kde=True)\n",
    "plt.axvline(np.median(error),color='r', linestyle='--')\n",
    "plt.title('Distribution of Error: Agglomerative Clustering', fontsize=18)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plt.xlabel('Error', fontsize=16)\n",
    "plt.ylabel('Count', fontsize=16)\n",
    "# plt.savefig(r\"../images/kmeans_error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-clothing",
   "metadata": {},
   "source": [
    "#### cos sim for items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-monroe",
   "metadata": {},
   "source": [
    "#### content similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "foreign-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nlp dict with list\n",
    "nlp_dict = defaultdict(list)\n",
    "\n",
    "# get unique IDs\n",
    "unique_asin = df['asin'].unique()\n",
    "\n",
    "# popupate dictionary with keys\n",
    "for asin in unique_asin:\n",
    "    nlp_dict[asin]\n",
    "    \n",
    "for idx, text in zip(df['asin'], df['summary']):\n",
    "    nlp_dict[idx].append(text)\n",
    "    \n",
    "# get unique descriptions and titles and dedupe\n",
    "desc_title = df[['asin', 'description', 'title']].drop_duplicates()\n",
    "    \n",
    "    \n",
    "for idx, text_1, text_2 in zip(desc_title['asin'], desc_title['description'], desc_title['title']):\n",
    "    nlp_dict[idx].append(text_1)\n",
    "    nlp_dict[idx].append(text_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "impossible-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set subdf with asin for mapping\n",
    "asin_df = df[['asin']].drop_duplicates()\n",
    "\n",
    "# map nlp dict to asin\n",
    "asin_df['text'] = asin_df['asin'].apply(lambda x: nlp_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "promising-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp preprocessing function\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "intimate-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set asin as index for easy reference and drop as a column\n",
    "asin_df.index = asin_df['asin']\n",
    "asin_df.drop('asin', axis=1, inplace=True)\n",
    "\n",
    "# run nlp cleaner over all text and create new column called clean\n",
    "asin_df['clean'] = asin_df['text'].map(lambda x: preprocess(x))\n",
    "\n",
    "# drop unprocessed text column\n",
    "asin_df.drop('text', axis=1, inplace=True)\n",
    "\n",
    "# save index to ordered list for easy future interpretation\n",
    "index_list = asin_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_df = tfidf.fit_transform(asin_df.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_final = pd.DataFrame(data=tfidf_df.todense(), index=asin_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_final.to_csv('../../product_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-concrete",
   "metadata": {},
   "source": [
    "#### User similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "driven-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set user dict with list\n",
    "user_dict = defaultdict(list)\n",
    "\n",
    "# get unique IDs\n",
    "unique_user = df['reviewerID'].unique()\n",
    "\n",
    "# popupate dictionary with keys\n",
    "for id in unique_user:\n",
    "    user_dict[id]\n",
    "    \n",
    "# get unique descriptions and titles and dedupe\n",
    "desc_title = df[['reviewerID', 'description', 'title']].drop_duplicates()\n",
    "    \n",
    "    \n",
    "for idx, text_1, text_2 in zip(desc_title['reviewerID'], desc_title['description'], desc_title['title']):\n",
    "    user_dict[idx].append(text_1)\n",
    "    user_dict[idx].append(text_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "restricted-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set subdf with asin for mapping\n",
    "user_df = df[['reviewerID']].drop_duplicates()\n",
    "\n",
    "# map nlp dict to asin\n",
    "user_df['text'] = user_df['reviewerID'].apply(lambda x: user_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "twelve-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set user as index for easy reference and drop as a column\n",
    "user_df.index = user_df['reviewerID']\n",
    "user_df.drop('reviewerID', axis=1, inplace=True)\n",
    "\n",
    "# run nlp cleaner over all text and create new column called clean\n",
    "user_df['clean'] = user_df['text'].map(lambda x: preprocess(x))\n",
    "\n",
    "# drop unprocessed text column\n",
    "user_df.drop('text', axis=1, inplace=True)\n",
    "\n",
    "# save index to ordered list for easy future interpretation\n",
    "index_list = user_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "given-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectorize the text\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_df = tfidf.fit_transform(user_df['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authentic-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later for ease of use\n",
    "user_final = pd.DataFrame(data=tfidf_df.todense(), index=user_df.index)\n",
    "\n",
    "user_final.to_csv('../../user_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "raised-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_final.to_csv('../../user_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-panel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "backed-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_final.to_csv('../../product_final.csv', index=False)\n",
    "# user_final.to_csv('../../user_final.csv', index=False)\n",
    "\n",
    "product_nlp = pd.read_csv('../../product_final.csv')\n",
    "user_nlp = pd.read_csv('../../user_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "million-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adaptive-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=100)\n",
    "# compressed_product = svd.fit_transform(product_nlp)\n",
    "# compressed_user = svd.fit_transform(user_nlp)\n",
    "\n",
    "# compressed_product_ = pd.DataFrame(data=compressed_product, index=asin_df.index)\n",
    "compressed_user_ = pd.DataFrame(data=compressed_user, index=user_df.index)\n",
    "\n",
    "\n",
    "# compressed_product_.to_csv('../../product_final.csv', index=False)\n",
    "compressed_user_.to_csv('../../user_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-hours",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "subject-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df.pivot_table(index='reviewerID', columns='asin', values='overall', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deluxe-yacht",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NMF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ebc64857db88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NMF' is not defined"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=100)\n",
    "nmf.fit(pivot_df)\n",
    "W = nmf.transform(pivot_df)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = (np.around(x,2) for x in (W,H))\n",
    "W = pd.DataFrame(W,index=pivot_df.index)\n",
    "H = pd.DataFrame(H,columns=pivot_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = H.iloc[0].sort_values(ascending=False).index[:3]\n",
    "top_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users = W.iloc[:,0].sort_values(ascending=False).index[:2]\n",
    "top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.loc['Emily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .2\n",
    "for g in range(100):\n",
    "    all_products = H.iloc[g,:]\n",
    "    included = H.columns[all_movies >= (thresh * all_products.max())]\n",
    "    print(\"Concept %i contains: %s\" % (g, ', '.join(included)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .2  # user is included if at least 20% of max weight\n",
    "for g in range(2):\n",
    "    all_users = W.iloc[:,g]\n",
    "    included = W.index[all_users >= (thresh * all_users.max())]\n",
    "    print(\"\\nConcept %i contains: %s\" % (g, ', '.join(included)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I am going to select a user and with NMF find their top 3 categories.\n",
    "I will find the top 10 products in each category that the user has not bought.\n",
    "I will then find others users that have bought that item and fill a list of users.\n",
    "I will then using cosine similarity, find the top 10 most similar users.\n",
    "I will then mean their ratings for the product and use that as my recomended ratings.\n",
    "I will then sort these items by the meaned ratings highest to lowest and recomend top 10.\n",
    "\n",
    "How do I test this?\n",
    "\n",
    "I will get users that have rated at least 50 items?\n",
    "use this method with items they have already bought and see what the MSE is.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "for each user, I want to find their top 3 categories.\n",
    "for each of these categories I want to find the top 10 products\n",
    "\n",
    "TOP 10 item for TOP 3 Cats (30 items total)\n",
    "\n",
    "For each of these items I want to find users that are most similar to our target user.\n",
    "How have they rated these items? I will mean their recomendation and call it as the targets recomendation.\n",
    "I will then sort the items from highest to lowest and recomend them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I then want to use their history to find out what items in these categories are most similar to what they bought\n",
    "but have not bought.\n",
    "\n",
    "I then want to recomend them the top 10 most similar items to their history\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_sim = cosine_similarity(user_final)\n",
    "\n",
    "def get_index_from_user(id):\n",
    "    return user_final[user_final. == title][\"index\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unlimited-credits",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (keras_predictions.py, line 207)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\walke\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3427\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-5bc804528e34>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from helper.keras_predictions import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\walke\\Desktop\\cap_3\\health_recommender\\src\\helper\\keras_predictions.py\"\u001b[1;36m, line \u001b[1;32m207\u001b[0m\n\u001b[1;33m    Image1 = Image.open(requests.get(url_lst[0]), stream=True).raw)\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "from helper.keras_predictions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "persistent-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "!python3 helper.karas_predictions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-noise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
